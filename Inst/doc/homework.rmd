---
title: "Homework 19072"
author: "By 19072"
date: "2020/1/1"
output: html_document
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



## Question 1
The Rayleigh density [156, Ch. 18] is
$$
f(x)=\frac{x}{\sigma^{2}} e^{-x^{2} /\left(2 \sigma^{2}\right)}, \quad x \geq 0, \sigma>0
$$
Develop an algorithm to generate random samples from a Rayleigh(σ) distribution.
Generate Rayleigh(σ) samples for several choices of σ > 0 and check
that the mode of the generated samples is close to the theoretical mode σ
(check the histogram).

## Question 2
Generate a random sample of size 1000 from a normal location mixture. The
components of the mixture have N(0, 1) and N(3, 1) distributions with mixing
probabilities p1 and p2 = 1 − p1. Graph the histogram of the sample with
density superimposed, for p1 = 0.75. Repeat with different values for p1
and observe whether the empirical distribution of the mixture appears to be
bimodal. Make a conjecture about the values of p1 that produce bimodal
mixtures.

## Question 3
Write a function to generate a random sample from a Wd(Σ, n) (Wishart)
distribution for n > d+ 1 ≥ 1, based on Bartlett’s decomposition.

## answer 1

```{r}
set.seed(1025)
par(mar=c(1,1,1,1))
par(mfrow=c(2,2))
n<-1.025e4;
sigma<-c(1,3,4,6);
for (i in 1:4)
{F<-runif(n)
x<-sqrt(-2*(sigma[i])^2*log(F))     ## F=exp(-x^2/(2*sigma^2))
hist(x,prob=TRUE,breaks=20,col="red",main=expression(f(x)==x/sigma^2*exp(-x^2/(2*sigma^2))))
y<-seq(0,20,0.001)
## Fit the curve of the theoretical model.
lines(y,y/(sigma[i])^2*exp(-y^2/(2*(sigma[i])^2)),col="blue") }

```

## answer 2

```{r}
set.seed(1225)
par(mar=c(1,1,1,1))
par(mfrow=c(3,3))
n<-1000
p1<-c(0.75,0.7,0.6,0.5,0.45,0.4,0.35,0.2,0.1)
p2<-1-p1
for (i in 1:9)
{
n1<-rbinom(1,n,p1[i]) 
##Generate n1 samples based on binomial distribution.
n2<-n-n1
x1<-rnorm(n1,0,1)
x2<-rnorm(n2,3,1)
x<-c(x1,x2)                ## combine x1 with x2
hist(x,prob=TRUE,breaks=15,col="brown",main=p1[i])
y<-seq(-4,6,0.01)
lines(y,p1[i]*(sqrt(2*pi))^(-1)*exp(-y^2/2)+p2[i]*(sqrt(2*pi)^(-1))*exp(-(y-3)^2/2),col="blue")
}
```


The empirical distribution of the mixture appears to be bimodal when p1 variates from 0.2 to 0.75.



## answer 3

```{r}
set.seed(1234)
miracle<-function(n,sigma)
{
d<-dim(sigma)
##Generate  a zero matrix T based on the dimesion of matrix sigma.
T<-matrix(0,nrow=d[1],ncol=d[2]) 
## Cholesky decomposition of matirx sigma produces an upper triangular matrix, so we need to transpose it to get the lower triangular matrix L. 
L<-t(chol(sigma))
## Construct a matrix T.
 for (i in 1:d[1] )
   {for (j in 1:d[2]) 
         {if (i>j)
            {T[i,j]<-rnorm(1,0,1)
             }
             else if (i<j)
               {
                   T[i,j]<-0
                 }
           else
                {
                    T[i,j]<-sqrt(rchisq(1,n-i+1))
                  }
           
   }

 }
A<-T%*%t(T)         
S<-L%*%A%*%t(L)
S
}
n<-10
sigma<-matrix(c(1,0.1,0.3,0.1,1,0.2,0.3,0.2,1),nrow=3)
x<-miracle(n,sigma)
x
```

## Question 1
Compute a Monte Carlo estimate of
$$
\int_{0}^{\pi / 3} \sin t d t
$$
and compare your estimate with the exact value of the integral.

## Question 2
Use Monte Carlo integration with antithetic variables to estimate
$$
\int_{0}^{1} \frac{e^{-x}}{1+x^{2}} d x
$$
and find the approximate reduction in variance as a percentage of the variance
without variance reduction.

## Question 3
Obtain the stratified importance sampling estimate in Example 5.13 and compare
it with the result of Example 5.10.

## Answer 1
```{r}
set.seed(12345678)
n<-3e6;
x<-runif(n,min=0,max=pi/3);
gamma.hat<-mean(sin(x))*pi/3            ## the value of the estimation
print(c(gamma.hat,-cos(pi/3)+cos(0)))   ## the value of the integration
```
## Answer 2
```{r}
set.seed(123456)
theta <- function(n =3e4, antithetic = TRUE) {
  a<- runif(n/2,0,1)
  if (!antithetic) b<-runif(n/2) else b<-1-a
  u <- c(a, b)
  theta<-numeric(1)
  g <-exp(-u)/(1+u^2)
  theta<- mean(g)
  theta
}

m <- 2e3
theta1 <- theta2 <- numeric(m)
for (i in 1:m) 
{
  theta1[i] <-theta(n =3e3, anti=FALSE)
  theta2[i] <-theta(n =3e3, anti=TRUE)
}
theta1.hat<-mean(theta1);        ## the estimation of integration without antithetic variables
theta2.hat<-mean(theta2);        ## the estimation of integration with antithetic variables
var1<-var(theta1);
var2<-var(theta2);
var12<-var2-var1;                 ## the difference of two estimations' variance
data.frame(theta1.hat,theta2.hat,var1,var2,var12)
```
The difference of two estimated variances are very small.

## Answer 3

```{r}
set.seed(1025)
N<-1e4;
K<-5;                ## the number of stratum
re<-N/K;
vector<-rep(0,K)
mir<- matrix(0, 50, 2)
g<-function(x)
  {exp(-x-log(1+x^2))}   ## g(x)
fg<-function(x)  
{
 g(x)/((exp(-x)/(1-exp(-1))))*(x<1)*(x>0)  ## fg(x)=g(x)/f(x)
}

for (i in 1:50) {
  for(j in 1:K)
    {
    Y<-runif(re,j-1,j)        ## sample from five equal interval between 0 and 5
    x<--log(1-Y/5*(1-exp(-1)))        ## Inverse transformation for X
    vector[j]<-mean(fg(x))          ## the estimation of per staturm
    }
    mir[i, 1]<-mean(vector)
    mir[i, 2]<-sd(vector)
}
apply(mir,2,mean)
```

## Question 1
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use aMonte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
$\chi^{2}(n)$ data with sample size n = 20. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)


## Question 2
Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b_{1}}$ under
normality by a Monte Carlo experiment. Compute the standard error of the
estimates from (2.14) using the normal approximation for the density (with
exact variance formula). Compare the estimated quantiles with the quantiles
of the large sample approximation $$\sqrt{b_{1}} \approx N(0,6 / n)$$


## Answer 1
```{r}
set.seed(1025)
n<-20
k<-3.65e3
N<-numeric(1)
XMLL<-XMUL<-numeric(k)
alpha<-0.05
for (i in 1:k)
{
x<-rchisq(n,2);
XMLL[i]<-mean(x)-qt(1-alpha/2,n-1)*sd(x)/sqrt(n)  
## the lower confidence limit of estimated parameter
XMUL[i]<-mean(x)+qt(1-alpha/2,n-1)*sd(x)/sqrt(n)
## the upper coonfidence limit of estimated parameter
if (XMLL[i]<=2 & XMUL[i]>=2)
 { N<-N+1}              ## the number of times that A falls into the confidence interval 
else
 { N<-N}
}
pvalue<-N/k             ## coverage probability 
pvalue
```
From the result above,you will find t-interval results of 0.9150685 is smaller than simulation results of 0.95 in Example 6.4 

## Answer 2
```{r}
set.seed(1025)
n<-1.3e3;
m<-1.3e3;
c<-c(0.025,0.05,0.95,0.975);
a<-a1<-a2<-a3<-sk<-numeric(n)
rq<-simq<-b<-sdsimq<-f<-numeric(4)
for (j in 1:n)
{
  x<-rnorm(m,0,sqrt(6/n))
  a1[j]<-mean(x)
  a2[j]<-mean((x-a1[j])^3)
  a3[j]<-(mean((x-a1[j])^2))^1.5
  sk[j]<-a2[j]/a3[j]
}
a<-sk[order(sk)]      
## order the estimated skewnesses we get from the lowest to the highest
for (i in 1:4)
{
  rq[i]<-qnorm(c[i],0,sqrt(6/n))
  b[i]<-c[i]*n                                    
  simq[i]<-a[b[i]]                               ## the quantiles of given normal distribution
  f[i]<-1/sqrt(2*pi*6/n)*exp(-(rq[i])^2/(2*6/n)) ## estimated quantiles
  sdsimq[i]<-sqrt((c[i]*(1-c[i]))/(n*(f[i])^2))  ## the standard error of estimated quantiles
}
data.frame(rq,simq,sdsimq)
```
With the samples n=1300 and m=1300,the difference between the estimated quantiles and quantiles of the large sample approximation is very small.

## Question 1
Estimate the power of the skewness test of normality against symmetric $\operatorname{beta}(\alpha,\alpha)$ distributions and comment on the results.Are the results different for heavy-tailed symmetric alternatives such as $\operatorname{t}(v)$?

## Question 2
6.A 
Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$,when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i)$\chi^{2}(1)$ (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $H_{0}: \mu=\mu_{0}\text { vs } H_{0}: \mu \neq \mu_{0}$ where $\mu_{0}$ is the mean of $\chi^{2}(1)$
,Uniform(0,2),and Exponential(1), respectively.

## Question 3
Disussion
 1.If we obain the powers for two methods undera particular simulation setting with 10000           experiments:say,0.651 for one method and 0.676 for another method. Can we say the powers
   are different at 0.05 level?
 2.What is the corresponding hythothesis test problem?
 3.What test should we use? Z-test,two sample t-test,paired-test or McNemar test?
 4.What information is needed to test your hypothesis?
 
## answer 1

### beta distribution with different alpha
```{r}
set.seed(1101)
n<-350
m<-1200
alpha<-c(seq(0.1,2,0.1),seq(1,73),2)
num1<-betaskew<-matrix(0,length(alpha),m)
p.reject<-cv<-numeric(length(alpha))
for (i in 1:length(alpha))
{
  for (j in 1:m)
{
  x<-rbeta(n,alpha[i],alpha[i])
  betaskew[i,j]<-mean((x-mean(x))^3)/((mean((x-mean(x))^2))^1.5)    
  ## the skewness of random statistic from beta distribution with alpha and beta equal 0.05
  cv[i]<-qnorm(0.975, 0,sqrt(6*(n-2)/((n+1)*(n+3))))                     
  ## critical value
  num1[i,j]<-as.integer(abs(betaskew[i,j])>=cv[i])       #test decision is 1 (reject) or 0)
 
}
p.reject[i]<-mean(num1[i,])
## the probability that the skewness falls in the reject domain for one loop
}
plot(alpha,p.reject,type="b",lty=5,pch=6,col="blue")
abline(h=.05,lty=4,col="red")
```

From the figure given above,you will find the p.reject has a trend of rising in the whole.Because beta distribution is also a symetric distribution and has a lighter tail when alpha increases, it is natural to get the result p.reject near to the nominal significance level when alpha is big,such alpha equals 50.

### t distribution with different degee of freedom 
```{r}
set.seed(1101)
n<-450
m<-1200
df<-seq(2,50,1)
num1<-betaskew<-matrix(0,length(df),m)
p.reject<-cv<-numeric(length(df))
for (i in 1:length(df))
{
  for (j in 1:m)
{
  x<-rt(n,df[i])
  betaskew[i,j]<-mean((x-mean(x))^3)/((mean((x-mean(x))^2))^1.5)    
  ## the skewness of random statistic from beta distribution with alpha and beta equal 0.05
  cv[i]<-qnorm(0.975, 0,sqrt(6*(n-2)/((n+1)*(n+3))))                     
  ## critical value
  num1[i,j]<-as.integer(abs(betaskew[i,j])>=cv[i])       #test decision is 1 (reject) or 0)
 
}
p.reject[i]<-mean(num1[i,])
## the probability that the skewness falls in the reject domain for one loop
}
plot(df,p.reject,type="b",lty=6,pch=8,col="red")
abline(h=.05,lty=3)
```

T distribution is a symmetric distribution with heavy tails and approximate noraml distribution when the defree of freedom is large enough,so when the defree of freedom increases,the p.reject is nearer to the alpha equals nominal significance level.

###  contaminated beta distribution 
```{r}
set.seed(1025)
n<-800
m<-1200
epsilon<-c(seq(0,0.1,0.01),seq(0.1,1,0.05))
num1<-betaskew<-matrix(0,length(epsilon),m)
pw<-cv<-numeric(length(epsilon))
for (i in 1:length(epsilon))
{
  for (j in 1:m)
{
  alpha<-sample(c(2,25),replace = TRUE,size=n,prob=c(1-epsilon[i],epsilon[i]))
  x<-rbeta(n,alpha,alpha)
  betaskew[i,j]<-mean((x-mean(x))^3)/((mean((x-mean(x))^2))^1.5)    
  ## the skewness of random statistic from beta distribution with alpha and beta equal 0.05
  cv[i]<-qnorm(0.975, 0,sqrt(6/n))                    
  ## critical value
  num1[i,j]<-as.integer(abs(betaskew[i,j])>=cv[i])       #test decision is 1 (reject) or 0)
 
}
pw[i]<-mean(num1[i,])
## the probability that the skewness falls in the reject domain for one loop
}
plot(epsilon,pw,type="b",lty=12,pch=    )
se <- sqrt(pw*(1-pw)/m)                #add standard errors
lines(epsilon, pw+se, lty = 6,col="blue")
lines(epsilon, pw-se, lty = 6,col="blue")
abline(h=.05,lty=2,col="brown")
```

From the image,you will find pw has a trend of rising when epsilon is smaller than about 0.9,then pw increase with the epsilon is larger than 0.9.The parameter alpha of distribution has a significant effect on the power function .When the alpha is large such as 25 given in the code which is larger the alpha is 2,the figure of pw is near to he right hand. Beacuse the bata distribution has a heavier tail with alpha is 2 than alpha is 25, the random data has a higher probability that fall in the two sides of distribution.

###  contaminated t distribution 
```{r}
set.seed(1101)
n<-250
m<-1200
epsilon<-c(seq(0,0.1,0.01),seq(0.1,1,0.05))
num1<-betaskew<-matrix(0,length(epsilon),m)
pw<-cv<-numeric(length(epsilon))
for (i in 1:length(epsilon))
{
  for (j in 1:m)
{
  df<-sample(c(2,50),replace = TRUE,size=n,prob=c(1-epsilon[i],epsilon[i]))
  x<-rt(n,df)
  betaskew[i,j]<-mean((x-mean(x))^3)/((mean((x-mean(x))^2))^1.5)    
  ## the skewness of random statistic from beta distribution with alpha and beta equal 0.05
  cv[i]<-qnorm(0.975, 0,sqrt(6*(n-2)/((n+1)*(n+3))))                    
  ## critical value
  num1[i,j]<-as.integer(abs(betaskew[i,j])>=cv[i])       #test decision is 1 (reject) or 0)
 
}
pw[i]<-mean(num1[i,])
## the probability that the skewness falls in the reject domain for one loop
}
plot(epsilon,pw,type="b",lty=4,pch=8,col="red",ylim =c(0,1))
se <- sqrt(pw*(1-pw)/m) #add standard errors
lines(epsilon, pw+se, lty = 6)
lines(epsilon, pw-se, lty = 6)
abline(h=.05,lty=2,col="blue")
```

T distribution has heavier tails when alpha is 50 than the alpha is 2 and T distribution approxiamate to the normal disribution when alpha is large enough,such as alpha equals 50,
so you will find the pw is near to 0.05 when epsilon is 1,that is to say,the power function of t distribution with degree of freedom is 50 is near to 0.05.You will also find the pw has a trend of decreasing in the whole.

## answer 2
Since the sample distribution is approximately normal when the sample is large enough, we perform data simulation from both large and small samples,n=1200,n=80 seperately.

### large sample n=1200
```{r}
set.seed(1025)
alpha<-c(0.01,0.025,0.05,0.1)
n<-1200                         ## the number of sample for one replicate
m<-1.35e3;                       ## the number of repilcates  
p1<-p2<-p3<-numeric(m)
p.hat1<-p.hat2<-p.hat3<-0
for (i in 1:m)
{
 x1<-rchisq(n,1) 
 x2<-runif(n,0,2)
 x3<-rexp(n,1)
 ## the means of above three distributions all are one.
 ttest1<-t.test(x1,alternative="two.side",mu=1)          
 ## the t-test of chi-square distribution
 ttest2<-t.test(x2,alternative="two.side",mu=1)  
 ## the t-test of uniform distribution
 ttest3<-t.test(x3,alternative="two.side",mu=1)
 ## the t-test of exponetion distribution
 p1[i]<-ttest1$p.value
 ## the simulation result of chi-square distribution for one alpha
 p2[i]<-ttest2$p.value
 ## the simulation result of uniform distributon for one alpha
 p3[i]<-ttest3$p.value
 # the simulation result of exponetion distribution for one alpha
 
}
for (j in 1:4)
{
p.hat1[j]<-round(mean(p1<=alpha[j]),3)
## the simulation result of chi-square distribution for four different alphas
p.hat2[j]<-round(mean(p2<=alpha[j]),3)
## the simulation result of uniform distributon for four different alphas
p.hat3[j]<-round(mean(p3<=alpha[j]),3)
# the simulation result of exponetion distribution for four different alphas
}
data.frame(alpha,chi.phat=p.hat1,unif.phat=p.hat2,exp.phat=p.hat3)
```

From the chart above,you will find chi.phat,unif.hat,exp.hat are mild different from the given alpha with the large sample n equals 1200,because the sample population approximates a normal  distribution when the the sample size is large enough.Thus,the empirical Type I error rate of the t-test is approximately equal to the nominal significance level alpha.


### small sample n=80
```{r}
set.seed(1025)
alpha<-c(0.01,0.025,0.05,0.1)
n<-80                         ## the number of sample for one replicate
m<-1.35e3;                       ## the number of repilcates  
p1<-p2<-p3<-numeric(m)
p.hat1<-p.hat2<-p.hat3<-0
for (i in 1:m)
{
 x1<-rchisq(n,1) 
 x2<-runif(n,0,2)
 x3<-rexp(n,1)
 ## the means of above three distributions all are one.
 ttest1<-t.test(x1,alternative="two.side",mu=1)          
 ## the t-test of chi-square distribution
 ttest2<-t.test(x2,alternative="two.side",mu=1)  
 ## the t-test of uniform distribution
 ttest3<-t.test(x3,alternative="two.side",mu=1)
 ## the t-test of exponetion distribution
 p1[i]<-ttest1$p.value
 ## the simulation result of chi-square distribution for one alpha
 p2[i]<-ttest2$p.value
 ## the simulation result of uniform distributon for one alpha
 p3[i]<-ttest3$p.value
 # the simulation result of exponetion distribution for one alpha
 
}
for (j in 1:4)
{
p.hat1[j]<-round(mean(p1<=alpha[j]),3)
## the simulation result of chi-square distribution for four different alphas
p.hat2[j]<-round(mean(p2<=alpha[j]),3)
## the simulation result of uniform distributon for four different alphas
p.hat3[j]<-round(mean(p3<=alpha[j]),3)
# the simulation result of exponetion distribution for four different alphas
}
data.frame(alpha,chi.phat=p.hat1,unif.phat=p.hat2,exp.phat=p.hat3)
```

From the chart above,you will find chi.phat,exp.hat are different from the given alpha with the large sample n equals 80,because three distribution are completly different from the normal distribution.The unif.hat has good results when alpha equals 0.010,0.025 but not good with alpha equals 0.05,0,1.


## answer 3
1.We can't say the powers are different at 0.05 level just for two results 0.651 and 0.676.We should get more information.

2.NUll hypothesis:          the powers of two methods are same
   Alternative hypothesis:   the powers of two methods are different
   
3.paired t test

4.The power obtained by one methods minus the power obtained by another method, and repeat the process for multiple times to obtain many sets of differences.With the size of sample increases,the difference of powers will  approximate a normal distribution.Construct a T statistic obeying the t distribution and use a paired t test to test Null hypothesis.

## Question 1
Efron and Tibshirani discuss the scor (bootstrap) test score data on 88 students
who took examinations in five subjects [84, Table 7.1], [188, Table 1.2.1].
The first two tests (mechanics, vectors) were closed book and the last three
tests (algebra, analysis, statistics) were open book. Each row of the data
frame is a set of scores $(\left(x_{i1},\dots,x_{i5}\right)$ for the $\ (i^{th})$ student. Use a panel display
to display the scatter plots for each pair of test scores. Compare the plot with
the sample correlation matrix. Obtain bootstrap estimates of the standard
errors for each of the following estimates:$\hat{\rho}_{12}=\hat{\rho}(\mathrm{mec},\mathrm{vec})$,$\hat{\rho}_{34}=\hat{\rho}(\mathrm{alg},\mathrm{ana})$,$\hat{\rho}_{35}=\hat{\rho}(\mathrm{alg},\mathrm{sta})$,$\hat{\rho}_{45}=\hat{\rho}(\mathrm{ana},\mathrm{sta})$


## Qusetion 2
Repeat Project 7.A for the sample skewness statistic. Compare the coverage
rates for normal populations (skewness 0) and $\chi^{2}(5)$ distributions (positive
skewness).


## answer 1
```{r}
library(bootstrap);
library(boot);
library(MASS);
set.seed(1025)
cor<-matrix(0,5,5)
opar<-par(no.readonly=TRUE)
par(mar=c(1,1,1,1))
par(mfrow=c(3,4))
for (i in 1:4)
{
  for (j in (i+1):5)
  {
    plot(t(scor[i]),t(scor[j]),pch=8,col="red",xlab=colnames(scor)[i],ylab=colnames(scor)[j])
  }
}
par(opar)

cor<-cor(scor)     ## coefficient mareix 
cor

set.seed(1025)
k<-45                 ## the size of one sample
a<-c(1,3,3,4)
b<-c(2,4,5,5)
original<-bias<-se<-numeric(4)
b.cor <-function(x1,i)
  { cor(x1[i,1],x1[i,2])}
for (m in 1:4)
{
  {
  x1<-mvrnorm(k,rep(0,2),matrix(c(cor[a[m],a[m]],cor[a[m],b[m]],cor[a[m],b[m]],cor[b[m],b[m]]),2))
  ## Generate random numbers from a binary normal distribution with a mean vector of (0,0)'and       a covariance matrix required.
  randnums <- boot(data=x1,statistic=b.cor,R=2500)
  original[m]<-round(randnums$t0,4)
  bias[m]<-round(mean(randnums$t)-randnums$t0,3)
  se[m]<-round(sd(randnums$t),3)
  }
}
estimated.parameters<-c("rho12","rho34","rho35","rho45")
data.frame(estimated.parameters,original,bias,se)
```

According the ten figures and the cofficient matrix,it will be seen that the higher the cofficient between two different vectors is,the better linear trend the plot of two different vectors has.


## answer 2

```{r}
set.seed(1025)
library(boot)
mu<-0
n<-20
interval.norm<-interval.basic<-interval.perc<-matrix(0,1000,2)
skew.f<- function(x,i) {
  #computes the sample skewness coeff.
  xbar <- mean(x[i])
  m3 <- mean((x[i] - xbar)^3)
  m2 <- mean((x[i] - xbar)^2)
  return( m3 / m2^1.5 )
}

for (i in 1:1000) {
  x<-rnorm(n,0,2)
  obj<-boot(x,statistic=skew.f,R=1000)
  inter<- boot.ci(obj,type=c("norm","basic","perc"))
  interval.norm[i,]<-inter$norm[2:3];
  interval.basic[i,]<-inter$basic[4:5];
  interval.perc[i,]<-inter$percent[4:5];
}
# the coverage probability of three types of interval
norm<-mean(interval.norm[,1]<=mu & interval.norm[,2]>=mu)
basic<-mean(interval.basic[,1]<=mu & interval.basic[,2]>=mu)
perc<-mean(interval.perc[,1]<=mu & interval.perc[,2]>=mu)
# the left side probability of three types of interval
norm.left<-mean(interval.norm[,1]>=mu)
basic.left<-mean(interval.basic[,1]>=mu)
perc.left<-mean(interval.perc[,1]>=mu)
# the right side probability of three types of interval
norm.right<-mean(interval.norm[,2]<=mu)
basic.right<-mean(interval.basic[,2]<=mu)
perc.right<-mean(interval.perc[,2]<=mu)
coverage.p<-c(norm,basic,perc)
left.p<-c(norm.left,basic.left,perc.right)
right.p<-c(norm.right,basic.right,perc.right)
distribution<-c("N(0,4)")
data.frame(distribution,coverage.p,left.p,right.p)
```


```{r}
set.seed(1025)
library(boot)
mu<-sqrt(8/5)
n<-20
interval.norm<-interval.basic<-interval.perc<-matrix(0,1000,2)
skew.f<- function(x,i) {
  #computes the sample skewness coeff.
  xbar <- mean(x[i])
  m3 <- mean((x[i] - xbar)^3)
  m2 <- mean((x[i] - xbar)^2)
  return( m3 / m2^1.5 )
}

for (i in 1:1000) {
  x<-rchisq(n,5)
  obj<-boot(x,statistic=skew.f,R=1200)
  inter<- boot.ci(obj,type=c("norm","basic","perc"))
  interval.norm[i,]<-inter$norm[2:3];
  interval.basic[i,]<-inter$basic[4:5];
  interval.perc[i,]<-inter$percent[4:5];
}
# the coverage probability of three types of interval
norm<-mean(interval.norm[,1]<=mu & interval.norm[,2]>=mu)
basic<-mean(interval.basic[,1]<=mu & interval.basic[,2]>=mu)
perc<-mean(interval.perc[,1]<=mu & interval.perc[,2]>=mu)
# the left side probability of three types of interval
norm.left<-mean(interval.norm[,1]>=mu)
basic.left<-mean(interval.basic[,1]>=mu)
perc.left<-mean(interval.perc[,1]>=mu)
# the right side probability of three types of interval
norm.right<-mean(interval.norm[,2]<=mu)
basic.right<-mean(interval.basic[,2]<=mu)
perc.right<-mean(interval.perc[,2]<=mu)
coverage.p<-c(norm,basic,perc)
left.p<-c(norm.left,basic.left,perc.right)
right.p<-c(norm.right,basic.right,perc.right)
distribution<-c("chi-square(5)")
data.frame(distribution,coverage.p,left.p,right.p)
```
According to the chart above,it will be found that chi-square distribution has a higher coverage probability of three types of bootstrap confidence interval compared with normal distribution with the the sample size is 20.At the same time,the left side probability of chis-quare is significant lower than the right side.


## Question 1
   Efron and Tibshirani discuss the following example [84,Ch.7]. The five-dimensional scores data have a 5 × 5 covariance matrix $\sum$,with positive eigenvalues ${\lambda}_{1}>\cdots>{\lambda}_{5}$ .In principal components analysis, $$\theta=\frac{\lambda_{1}}{\sum_{j=1}^{5} \lambda_{j}}$$ measures the proportion of variance explained by the first principal component.Let $\hat{\lambda}_{1}>\cdots>\hat{\lambda}_{5}$ be the eigenvalues of $$\hat{\sum}$$,where $$\hat{\sum}$$ is the MLE of $\sum$.
Compute the sample estimate 
$$ \hat{\theta}=\frac{ \hat{\lambda}_{1}}{\sum_{j=1}^{5} \hat{\lambda}_{j}} $$
   Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.
   
## Question 2
In Example 7.18, leave-one-out (n-fold) cross validation was used to select
the best fitting model. Repeat the analysis replacing the Log-Log model
with a cubic polynomial model. Which of the four models is selected by the
cross validation procedure? Which model is selected according to maximum
adjusted $R^{2}$?

## answer 1
```{r}
library(bootstrap)
n <- 88
theta.hat <- numeric(n)
cov <- cov(scor)
 ## covariance matrix
eig<- (eigen(cov))$values
 ## eigenvalues computed from the covariance matrix
eigen<- eig[order(-(eig))]
 ## order the eigenvalues from the largest to the lowest
theta <- eigen[1]/sum(eigen)
for (i in 1:n) 
{
  scor.jack <- scor[-i, ]
  cov.hat <- cov(scor.jack)
  ## the jackknife estimates of covariance matrix 
  eig.jack<- (eigen(cov.hat))$values
  ## the jackknife estimates of eigenvalues computed from the covariance matrix
  eigen.jack<- eig.jack[order(-(eig.jack))]
   ## order the eigenvalues from the largest to the lowest
  theta.hat[i] <- eigen.jack[1]/sum(eigen.jack)
}
bias <- (n-1)*(mean(theta.hat)-theta)
se <- sqrt((n-1)*mean((theta.hat-theta)^2))
round(c(original = theta, bias = bias, se = se), 5)

```
According to the result,we will find that the jackknife estimates of bias and standard error of $\hat{\theta}$ are both small.

## answer 2
```{r}
library(DAAG,quietly=TRUE);
attach(ironslag)
n <- length(magnetic)   #in DAAG ironslag
error1 <- error2 <- error3 <- error4 <- numeric(n)
y.hat1 <- y.hat2 <- y.hat3 <- y.hat4 <- numeric(n)
R1 <- R2 <- R3 <- R4 <- numeric(n)
T1 <- T2 <- T3 <- T4 <- numeric(n)
ymean <-  mean(magnetic)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
  y <- magnetic[-k]
  x <- chemical[-k]
  ## linear model
  model1 <- lm(y ~ x)
  y.hat1[k] <- model1$coef[1] + model1$coef[2] * chemical[k]
  error1[k] <- magnetic[k] - y.hat1[k]
  R1[k] <- (y.hat1[k]-ymean)^2
  T1[k] <- (magnetic[k]-ymean)^2
  
  ## quadratic model
  model2 <- lm(y ~ x + I(x^2))
  y.hat2[k] <- model2$coef[1] + model2$coef[2] * chemical[k] +model2$coef[3] *    chemical[k]^2
  error2[k] <- magnetic[k] - y.hat2[k]
  R2[k] <- (y.hat2[k]-ymean)^2
  T2[k] <- (magnetic[k]-ymean)^2
  
  ## log-log model
  model3 <- lm(log(y) ~ x)
  y.hat3[k] <- exp(model3$coef[1] + model3$coef[2] * chemical[k])
  error3[k] <- magnetic[k] - y.hat3[k]
  R3[k] <- (y.hat3[k]-ymean)^2
  T3[k] <- (magnetic[k]-ymean)^2
  
  ## cubic polynomial model
  model4 <- lm(y ~ x + I(x^2) + I(x^3))
  y.hat4[k] <- model4$coef[1] + model4$coef[2] * chemical[k] +model4$coef[3] * chemical[k]^2 + model4$coef[4] * chemical[k]^3
  error4[k] <- magnetic[k] - y.hat4[k]
  R4[k] <- (y.hat4[k]-ymean)^2
  T4[k] <- (magnetic[k]-ymean)^2
}
error.square<-c(mean(error1^2), mean(error2^2), mean(error3^2), mean(error4^2))
adjusted.Rsquare<-c(sum(R1)/sum(T1)*51/50, sum(R2)/sum(T2)*51/49, sum(R3)/sum(T3)*51/50, sum(R4)/sum(T4)*51/48)
## adjusted R.square=(SSE/n-p-1)/(SST/n-1) ,n=52
models<-c("linear","quadratic","log-log","cubic")
data.frame(models,error.square,adjusted.Rsquare)

```
   In the four models given above,the value of $e^{2}$ about the quadratic model is the smallest,so it is the best fitting model selected by the cross validation procedure.
   AS for the adjusted $R^{2}$, the closer the adjusted $R^{2}$ value is to 1, the better the simulation effect,so naturally,we think the second model is the best fitting model.
   
   ## Question 1
The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

## Question 2
Power comparison (distance correlation test versus ball covariance test)
  Model 1: Y = X/4 + e;
  Model 2: Y = X/4 × e.
$$ X \sim N\left(0_{2}, l_{2}\right), e \sim N\left(0_{2}, l_{2}\right) $$,X and e are independent.


## answer 1
```{r}
set.seed(1205)
n<-85
m<-90
N<-1000
extnum<-seq(5,20,1)
p0<-numeric(length(extnum))
p<-numeric(length(extnum))
extp<-numeric(N)
x<-rnorm(n,0,3)
y<-rnorm(m,5,3)
z <- c(x, y);
K <- 1:(n+m);

count5test <- function(x, y,extnum)
  {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) >extnum ))
}


 for (i in 1:length(extnum)) 
 {
 p0[i]<-count5test(x, y,extnum[i])
 for (j in 1:N)
 {
 k <- sample(K, size = n, replace = FALSE)
 x1 <- z[k]
 y1 <- z[-k] #complement of x1
 x1<-x1-mean(x1)
 y1<-y1-mean(y1)
 extp[j]<-count5test(x1, y1,extnum[i])
 }
 p[i]<-mean(abs(c(p0[i],extp)))
 }
data.frame(extnum,p)

plot(extnum,p,type="b",pch=8,col="red",xlab = "the number of the extreme points",ylab="p-value")

```

From the figure above,we can find that with the number of extreme points increse, the p-values  will decrese when sample sizes are fixed which are 85 and 95.

## answer 2
```{r}
library(boot)
library(MASS)
library(Ball)
alpha<-0.05
dCov <- function(x, y) { 
  x <- as.matrix(x);  y <- as.matrix(y)
  n <- nrow(x); m <- nrow(y)
  if (n != m || n < 2) stop("Sample sizes must agree") 
  if (! (all(is.finite(c(x, y)))))
    stop("Data contains missing or infinite values")
  Akl <- function(x) {
    d <- as.matrix(dist(x))
    m <- rowMeans(d); M <- mean(d)
    a <- sweep(d, 1, m); b <- sweep(a, 2, m) 
    b + M
  }
  A <- Akl(x);  B <- Akl(y)
  sqrt(mean(A * B)) 
}
```

```{r}
ndCov2 <- function(z, ix, dims) {
  #dims contains dimensions of x and y 
  p <- dims[1]
  q <- dims[2]
  d <- p + q
  x <- z[ , 1:p] #leave x as is
  y <- z[ix, -(1:p)] #permute rows of y 
  return(nrow(z) * dCov(x, y)^2)
}
```


```{r}
num<-seq(20,200,20)
p.cor1<-p.cor2<-p.ball1<-p.ball2<-numeric(100)
pc.pw1<-pc.pw2<-pb.pw1<-pb.pw2<-numeric(length(num))
for (i in 1:length(num))
{
 for (j in 1:50)
 {
  x0<-mvrnorm(num[i],c(0,0),matrix(c(1,0,0,1),ncol=2))
  e<-mvrnorm(num[i],c(0,0),matrix(c(1,0,0,1),ncol=2))
  y1<-x0/4+e
  y2<-x0/4*e
  z1<- cbind(x0,y1)
  z2<- cbind(x0,y2)
  boot.obj1 <- boot(data = z1, statistic = ndCov2, R = 99,
                 sim = "permutation", dims = c(2, 2))
  boot.obj2 <- boot(data = z2, statistic = ndCov2, R = 99,
                 sim = "permutation", dims = c(2, 2))
  tb1 <- c(boot.obj1$t0, boot.obj1$t)
  tb2 <- c(boot.obj2$t0, boot.obj2$t)
  p.cor1[j] <- mean(tb1>=tb1[1])
  p.cor2[j] <- mean(tb2>=tb2[1])
  p.ball1[j] <- bcov.test(z1[,1:2],z1[,3:4],R=99,seed=i*j*2)$p.value
  p.ball2[j] <- bcov.test(z2[,1:2],z2[,3:4],R=99,seed=i*j*4)$p.value
 }
  pc.pw1[i]<-mean(p.cor1<=alpha)
  ## model1:the power of distance correlation test
  pb.pw1[i]<-mean(p.ball1<=alpha)
  ## model2:the power of ball covariance test
  pc.pw2[i]<-mean(p.cor2<=alpha)
   ## model2:the power of distance correlation test
  pb.pw2[i]<-mean(p.ball2<=alpha)
  ## model2:the power of ball covariance test
}
plot(num,pc.pw1,type="b",pch=8,col="red",xlab="sample size",ylab="power",main="model1:distance versus ball")
lines(num,pb.pw1,type="b",pch=6,col="blue")
legend("bottomright",c("distance","ball"),lty=c(1,2),pch=c(8,6),col=c("red","blue"))
plot(num,pc.pw2,type="b",pch=5,col="brown",xlab="sample size",ylab="power",main="model2:distance versus ball")
lines(num,pb.pw2,type="b",pch=4,col="green")
legend("bottomright",c("distance","ball"),lty=c(1,2),pch=c(5,4),col=c("brown","green"))
```


From the figures above,we will find that as the sample size increases,two powers both show an upward trend.In the first figure,the power values of the distance correlation tests are higher than that of the ball covariance tests, but the second figure is oppsite from the first one.

## Question 1
   Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.


## answer 1
```{r}
set.seed(1025)
    rw.Metropolis <- function(sigma, x0, N) {
        x <- numeric(N)
        x[1] <- x0
        u <- runif(N)
        k <- 0
        for (i in 2:N) {
            y <- rnorm(1, x[i-1], sigma)
                if (
                    u[i] <= ((1/2*exp(-abs(y)))/(1/2*exp(-abs(x[i-1])))))
        # the density of Laplace function f(x)=1/2*exp(-abs(x)
                    x[i] <- y  
                else {
                    x[i] <- x[i-1]
                    k <- k + 1
                }
            }
        return(list(x=x, k=k))
    }

    n <- 4  #degrees of freedom for target Student t dist.
    N <- 4000
    sigma <- c(.05, .5, 6,  20)

    x0 <- 10
    rw1 <- rw.Metropolis(sigma[1], x0, N)
    rw2 <- rw.Metropolis(sigma[2], x0, N)
    rw3 <- rw.Metropolis(sigma[3], x0, N)
    rw4 <- rw.Metropolis(sigma[4], x0, N)

    #number of candidate points rejected
    reject=c(rw1$k, rw2$k, rw3$k, rw4$k)
    reject.rate<-reject/N
    accept<-N-reject
    accept.rate<-accept/N
    ABC <- data.frame(sigma=sigma,accept,accept.rate,reject,reject.rate)
    knitr::kable(ABC)
```

From the from above,you will find that the acceptance rate is in the [0.15,0.5] 
when standard derivation equals 6.

```{r}

    par(mar=c(1,1,1,1))
    par(mfrow=c(2,2))  #display 4 graphs together
    refline <- c(log(0.05),-log(0.05))
    #  Use inverse function to find quantiles
    rw <- cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
    for (j in 1:4) {
        plot(rw[,j], type="l",
             xlab=bquote(sigma == .(round(sigma[j],3))),
             ylab="X", ylim=range(rw[,j]))
        abline(h=refline)
    }
    par(mar=c(1,1,1,1))
    par(mfrow=c(1,1)) #reset to default
    
```
```{r}
a <- c(.05, seq(.1, .9, .1), .95)
    Q1<-log(2*a[1:6])      
    #  Use inverse function to find quantiles
    Q2<--log(2-2*a[7:11])
    Q <- c(Q1,Q2)
    rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
    mc <- rw[501:N, ]
    Qrw <- apply(mc, 2, function(x) quantile(x, a))
    ABC <- data.frame(round(cbind(Q, Qrw), 3))
    names(ABC) <- c('True','sigma=0.05','sigma=0.5','sigma=2','sigma=16')
    knitr::kable(ABC) 
```

Combinig the four figures and form above,we will find that compare the chain  generated when different variances are used for the proposal distribution,thata equals 2 has best performance on the whole.

## Question 1
The natural logarithm and exponential functions are inverses of each other,
so that mathematically log(exp x) = exp(logx) = x. Show by example that
this property does not hold exactly in computer arithmetic. Does the identity
hold with near equality? (See all.equal.)

## Question 2
Write a function to solve the equation 
$$
\begin{aligned} \frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1) \Gamma\left(\frac{k-1}{2}\right)}} \int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k / 2} d u & \\=& \frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1) / 2} d u \end{aligned}
$$
for $a$,where
$$ c_{k}=\sqrt{\frac{a^{2} k}{k+1-a^{2}}} $$
Compare the solutions with the points $A(k)$ in Exercise 11.4.

## Question 3
A-B-O blood type problem
Let the three alleles be A, B and O with allele frequencies p, q and r. The 6 genotype frequencies under HWE and complete counts are as follows.

Genotype       AA   BB   OO   AO   BO   AB   Sum

Frequency      p^2  q^2  r^2  2pr  2qr  2pq   1

Count          nAA  nBB  nOO  nAO  nBO  nAB   n

Observed data: nA· = nAA + nAO = 28 (A-type),
nB· = nBB + nBO = 24 (B-type), nOO = 41 (O-type),nAB = 70 (AB-type).

1. Use EM algorithm to solve MLE of p and q (consider missing data nAA and nBB).

2. Show that the log-maximum likelihood values in M-steps are increasing via line plot.



## answer 1
```{r}
x<-seq(2,16,2)
a<-log(exp(x))
b<-exp(log(x))
a==b
a==x
b==x
isTRUE(all.equal(a,b))
```

From the result above,log(exp(x)) = exp(log(x)) does not hold exactly in computer arithmetic when x euqls 8,10,14,16,but the identity hold with near equality.


## answer 2
```{r}
k<-c(5:25,100) ##The root of equation hasn't been solved when k equals 4,so the loop starts when k equals 5
root<-numeric(length(k))
for (i in 1:length(k))
{
  coef<-function(k)
  {
   return(gamma((k+1)/2)/(gamma(k/2)*sqrt(k*pi)))
    ## the coefficient of integral
  }
 f<-function(a)
{
(coef(k[i])*(integrate(function(x)(1+(x^2)/k[i])^(-(1+k[i])/2),lower=sqrt((a^2)*k[i]/(k[i]+1-a^2)),upper=Inf)$value)- coef(k[i]-1)*(integrate(function(x) (1+(x^2)/(k[i]-1))^(-k[i]/2),lower=sqrt((a^2)*(k[i]-1)/(k[i]-a^2)),upper=Inf)$value))
}
root[i]<-uniroot(f,lower =0.01,upper = 1+sqrt(k[i])/2)$root
}
data.frame(k,root)
```



```{r}
k<-c(4:25,100)
## ##The root of equation hasn't been solved when k equals 500 and 1000,so the loop starts from k equals 4 to k equals 100.
root<-numeric(length(k))
for (i in 1:length(k))
{
  coef<-function(k)
  {
   return(gamma((k+1)/2)/(gamma(k/2)*sqrt(k*pi)))
    ## the coefficient of integral
  }
 f<-function(a)
{
(coef(k[i])*(integrate(function(x)(1+(x^2)/k[i])^(-(1+k[i])/2),lower=0,upper=sqrt((a^2)*k[i]/(k[i]+1-a^2)))$value)- coef(k[i]-1)*(integrate(function(x) (1+(x^2)/(k[i]-1))^(-k[i]/2),lower=0,upper=sqrt((a^2)*(k[i]-1)/(k[i]-a^2)))$value))
}
root[i]<-uniroot(f,lower =0.01,upper = 1+sqrt(k[i])/2)$root
}
data.frame(k,root)
```
By comparing the two tables, we can findthat when the root can be solved, the value of the root is the same when the k is given.





## answer 3
```{r}
p <- q <- r <- numeric(1000)
nA. <- 28; nB. <- 24; nOO <- 41; nAB <- 70
p[1] <- 0.09; q[1] <- 0.23;r[1] <- (1- p[1]- q[1])
threshold <- 2e-6


f <- function(a,b) {
  return((nB.*b/(2-b-2*a)+nB.+nAB)/(nA.*a/(2-a-2*b)+nA.+nAB))
}
g <- function(a,b) {
 return(((1-a/(2-a-2*b))*nA.+(1-b/(2-b-2*a))*nB.+2*nOO)/((nB.*b/(2-b-2*a)+
                          nB.+nAB)))
}

for (i in 2:1000)
   {
   p[i] <- 1/(1+f(p[i-1],q[i-1])*(1+g(p[i-1],q[i-1])))
   ## the maximum likelihood estimation of p
   q[i] <- f(p[i-1],q[i-1])/(1+f(p[i-1],q[i-1])*(1+g(p[i-1],q[i-1])))
   ## the maximum likelihood estimation of q
   r[i] <- 1- p[i] - q[i]
  
   if((p[i]-p[i-1] <= threshold) & (q[i]-q[i-1] <= threshold) &
      (r[i]-r[i-1] <= threshold))
   ##  the loop will stop if the p's and q's differece of values between two   iterations are both samller the given threshold.
       {print(c(i, p[i], q[i],r[i]))
       break
    }
}
x <-c(1:i)
plot(x, p[1:i], type="b",pch=8, col = "purple",ylim=c(0,0.7), main = "The log-maximum likelihood values in M-steps" ,lty = 4, xlab = "The number of iterations", ylab = "The parameters' value of iterations")
lines(x, q[1:i], type= "b", pch=11,col = "red",lty = 5)
lines(x, r[1:i], type= "b", pch=6, col = "blue",lty = 6)
legend("bottomright", legend = c("p", "q", "r"),pch=c(8,11,6),lty =c(4,5,6), col = c("purple", "red", "blue"))
```

From the figure above,we will find that the log-maximum likelihood values of p and q are both incresing via line plot.The value of p is converges to 0.327,and q is converges to 0.310.

## Question 1
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

   formulas <- list(
   mpg ~ disp,
   mpg ~ I(1 / disp),
   mpg ~ disp + wt,
   mpg ~ I(1 / disp) + wt
   )

## answer 1
```{r}
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
a<-numeric(4)
for (i in seq_along(formulas))
{
a[i]<-lapply(i,function(i) {lm(formulas[[i]],data=mtcars)})
}
a

```

## Question 2
Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply().Can you do it without an anonymous function?

       bootstraps <- lapply(1:10, function(i) {
           rows <- sample(1:nrow(mtcars), rep = TRUE)
           mtcars[rows, ]
       })
  
## answer 2       
```{r}
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
lapply(bootstraps, lm, formula = mpg ~ disp)

```
```{r}
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
b<-numeric(10)
for (i in 1:10)
{
  b[i]<-lapply(bootstraps[i], lm, formula = mpg ~ disp)

}
b
```


## Question 3
For each model in the previous two exercises, extract R2 using
the function below.

rsq <- function(mod) summary(mod)$r.squared

## answer 3
```{r}
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
a<-numeric(4)
rsq <- function(mod) summary(mod)$r.squared
for (i in 1:4){
  a[i]<-lapply(i,function(i){rsq(lm(formulas[[i]],data=mtcars))})
}
a
```
```{r}

bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
rsq <- function(mod) summary(mod)$r.squared

coe<-numeric(10)
for (i in 1:10)
{
 coe[i]<-lapply(i,function(i){rsq(lm(mpg~disp,data=bootstraps[[i]]))})
}
coe

```

## Question 4
The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.

   trials <- replicate(100,
           t.test(rpois(10, 10), rpois(7, 10)),
           simplify = FALSE
           )

## answer 4

```{r}
trials <- replicate(100,
           t.test(rpois(10, 10), rpois(7, 10)),
           simplify = FALSE
           )
sapply(1:100, function(i) {trials[[i]]$p.value})   
```


```{r}
sapply(trials, "[[",3)
```

## Question 5
Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()? Why or why not?

## answer 5
```{r}
library(parallel)
# mcsapply()
mcsapply<-function(k,f){
cl <- makeCluster(getOption("cl.cores", 4))
result<-parLapply(cl,k,f) 
stopCluster(cl) 
return(unlist(result))
} 
trials <- replicate(
         2000,
         t.test(rnorm(11,1,3), rnorm(12,2,4)),
         simplify = FALSE
       )
system.time(mcsapply(trials,function(x) unlist(x)[3]))
```

```{r}
system.time(sapply(trials,function(x) unlist(x)[3]))
```

Comparing the above two functions, you will find that the ncsapply function runs more efficiently than sapply function.

I can't implement mcvapply(), a parallel version of vapply(),beacuse vapply() directly output non-list.



```{r}
library(Rcpp)
library(microbenchmark)

set.seed(1025)
rw_MetropolisR <- function(sigma, x0, N) 
{
  #Metropolis Randomwalk using R
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= exp(-(abs(y) - abs(x[i-1]))))
      x[i] <- y else {
        x[i] <- x[i-1]
        k <- k + 1
      }
  }
  return(list(x=x, k=k))
}
```

```{r}
library(Rcpp)
#// This is the rw_MetropolisC.cpp
#include <Rcpp.h>
#using namespace Rcpp;
#// [[Rcpp::export]]
cppFunction('NumericVector rw_MetropolisC(double sigma, double x0, int N) 
{
  //Metropolis Randomwalk using C
  NumericVector x(N);
  x[0] = x0;
  double u, y;
  int k = 0;
  for (int i = 1; i < N; i++) 
  {
    y = rnorm(1, x[i-1], sigma)[0];
    u = runif(1)[0];
    if (u <= exp(-(abs(y) - abs(x[i-1])))) 
    {
      x[i] = y; 
    }
    else 
    {
      x[i] = x[i-1];
      k++;
    }
  }
  return x;
}')
```



```{r}
N = 4000
sigma <- c(0.5,12,60)
x0 = 12
for (i in 1:length(sigma)){
ts = microbenchmark(rwR = rw_MetropolisR(sigma[i], x0, N)$x, 
                    rwC = rw_MetropolisC(sigma[i], x0, N))
print(summary(ts)[, c(1,3,5,6)])

rwR = rw_MetropolisR(sigma[i], x0, N)$x
rwC = rw_MetropolisC(sigma[i], x0, N)
par(mar=c(1,1,1,1))
par(mfrow = c(1, 3))
b <- 1001 #discard the burnin sample
y <- (rwR)[b:N]
a <- ppoints(100)
QR <- ifelse(a <= 1/2, log(2*a), -log(2-2*a)) #quantiles of Laplace
QQ1 <- quantile(rwR, a)
qqplot(QR, QQ1, main=paste("R sigma=",sigma[i],seq=NULL), xlab="Laplace Quantiles", ylab="Sample Quantiles",col="blue")
abline(a=0, b=1)

y <- (rwC)[b:N]
a <- ppoints(100)
QR <- ifelse(a <= 1/2, log(2*a), -log(2-2*a)) #quantiles of Laplace
QQ2 <- quantile(rwC, a)
qqplot(QR, QQ2, main=paste("C sigma=",sigma[i],seq=NULL), xlab="Laplace Quantiles", ylab="Sample Quantiles",col="purple")
abline(a=0, b=1)

qqplot(QQ1, QQ2, main=paste("C-R sigma=",sigma[i],seq=NULL), xlab="R Samplee Quantiles", ylab="C Sample Quantiles",col="red")
abline(a=0, b=1)
}
```

From the figures above,We will find that Rcpp function's results are generally the same as R function's results with different sigmas. 
However, compared with R function,Rcpp function takes less computational time. 
